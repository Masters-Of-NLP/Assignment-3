{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIr4WK4iBy92"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model_pretrained = AutoModel.from_pretrained(\"PinkiKumari22/FinalPreTrainedModel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_finetuned_classification = AutoModelForSequenceClassification.from_pretrained(\"PinkiKumari22/finetuned_classification\", num_labels=2)\n"
      ],
      "metadata": {
        "id": "KuBKVwM3CTGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_finetuned_classification = AutoModelForSequenceClassification.from_pretrained(\"PinkiKumari22/finetuned_classification\", num_labels=2)\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model_finetuned_QA = AutoModelForQuestionAnswering.from_pretrained(\"PinkiKumari22/finetuned_qa\", from_tf=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeIdK7u6CJI3",
        "outputId": "c7e45e36-7b77-40a6-aa6f-542402b8cb50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "All the weights of BertForQuestionAnswering were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model_pretrained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcB1sWO_DCV-",
        "outputId": "ae0777c1-a9ce-4d59-c965-5d7276860d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------+------------+\n",
            "|                      Modules                       | Parameters |\n",
            "+----------------------------------------------------+------------+\n",
            "|         embeddings.word_embeddings.weight          |  23440896  |\n",
            "|       embeddings.position_embeddings.weight        |   393216   |\n",
            "|      embeddings.token_type_embeddings.weight       |    1536    |\n",
            "|            embeddings.LayerNorm.weight             |    768     |\n",
            "|             embeddings.LayerNorm.bias              |    768     |\n",
            "|    encoder.layer.0.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.0.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.0.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.0.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.0.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.0.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.0.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.0.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.0.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.0.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.0.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.0.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.0.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.0.output.dense.bias          |    768     |\n",
            "|      encoder.layer.0.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.0.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.1.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.1.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.1.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.1.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.1.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.1.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.1.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.1.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.1.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.1.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.1.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.1.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.1.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.1.output.dense.bias          |    768     |\n",
            "|      encoder.layer.1.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.1.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.2.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.2.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.2.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.2.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.2.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.2.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.2.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.2.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.2.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.2.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.2.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.2.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.2.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.2.output.dense.bias          |    768     |\n",
            "|      encoder.layer.2.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.2.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.3.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.3.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.3.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.3.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.3.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.3.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.3.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.3.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.3.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.3.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.3.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.3.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.3.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.3.output.dense.bias          |    768     |\n",
            "|      encoder.layer.3.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.3.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.4.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.4.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.4.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.4.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.4.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.4.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.4.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.4.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.4.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.4.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.4.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.4.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.4.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.4.output.dense.bias          |    768     |\n",
            "|      encoder.layer.4.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.4.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.5.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.5.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.5.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.5.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.5.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.5.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.5.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.5.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.5.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.5.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.5.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.5.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.5.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.5.output.dense.bias          |    768     |\n",
            "|      encoder.layer.5.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.5.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.6.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.6.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.6.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.6.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.6.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.6.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.6.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.6.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.6.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.6.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.6.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.6.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.6.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.6.output.dense.bias          |    768     |\n",
            "|      encoder.layer.6.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.6.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.7.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.7.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.7.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.7.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.7.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.7.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.7.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.7.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.7.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.7.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.7.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.7.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.7.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.7.output.dense.bias          |    768     |\n",
            "|      encoder.layer.7.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.7.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.8.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.8.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.8.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.8.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.8.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.8.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.8.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.8.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.8.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.8.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.8.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.8.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.8.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.8.output.dense.bias          |    768     |\n",
            "|      encoder.layer.8.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.8.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.9.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.9.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.9.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.9.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.9.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.9.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.9.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.9.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.9.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.9.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.9.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.9.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.9.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.9.output.dense.bias          |    768     |\n",
            "|      encoder.layer.9.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.9.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.10.attention.self.query.weight    |   589824   |\n",
            "|     encoder.layer.10.attention.self.query.bias     |    768     |\n",
            "|     encoder.layer.10.attention.self.key.weight     |   589824   |\n",
            "|      encoder.layer.10.attention.self.key.bias      |    768     |\n",
            "|    encoder.layer.10.attention.self.value.weight    |   589824   |\n",
            "|     encoder.layer.10.attention.self.value.bias     |    768     |\n",
            "|   encoder.layer.10.attention.output.dense.weight   |   589824   |\n",
            "|    encoder.layer.10.attention.output.dense.bias    |    768     |\n",
            "| encoder.layer.10.attention.output.LayerNorm.weight |    768     |\n",
            "|  encoder.layer.10.attention.output.LayerNorm.bias  |    768     |\n",
            "|     encoder.layer.10.intermediate.dense.weight     |  2359296   |\n",
            "|      encoder.layer.10.intermediate.dense.bias      |    3072    |\n",
            "|        encoder.layer.10.output.dense.weight        |  2359296   |\n",
            "|         encoder.layer.10.output.dense.bias         |    768     |\n",
            "|      encoder.layer.10.output.LayerNorm.weight      |    768     |\n",
            "|       encoder.layer.10.output.LayerNorm.bias       |    768     |\n",
            "|    encoder.layer.11.attention.self.query.weight    |   589824   |\n",
            "|     encoder.layer.11.attention.self.query.bias     |    768     |\n",
            "|     encoder.layer.11.attention.self.key.weight     |   589824   |\n",
            "|      encoder.layer.11.attention.self.key.bias      |    768     |\n",
            "|    encoder.layer.11.attention.self.value.weight    |   589824   |\n",
            "|     encoder.layer.11.attention.self.value.bias     |    768     |\n",
            "|   encoder.layer.11.attention.output.dense.weight   |   589824   |\n",
            "|    encoder.layer.11.attention.output.dense.bias    |    768     |\n",
            "| encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n",
            "|  encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n",
            "|     encoder.layer.11.intermediate.dense.weight     |  2359296   |\n",
            "|      encoder.layer.11.intermediate.dense.bias      |    3072    |\n",
            "|        encoder.layer.11.output.dense.weight        |  2359296   |\n",
            "|         encoder.layer.11.output.dense.bias         |    768     |\n",
            "|      encoder.layer.11.output.LayerNorm.weight      |    768     |\n",
            "|       encoder.layer.11.output.LayerNorm.bias       |    768     |\n",
            "|                pooler.dense.weight                 |   589824   |\n",
            "|                 pooler.dense.bias                  |    768     |\n",
            "+----------------------------------------------------+------------+\n",
            "Total Trainable Params: 109482240\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109482240"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model_finetuned_QA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPe7ULGcDCGs",
        "outputId": "63e6b653-d4de-4c55-9a81-2a8b119b6ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------+------------+\n",
            "|                         Modules                         | Parameters |\n",
            "+---------------------------------------------------------+------------+\n",
            "|          bert.embeddings.word_embeddings.weight         |  23440896  |\n",
            "|        bert.embeddings.position_embeddings.weight       |   393216   |\n",
            "|       bert.embeddings.token_type_embeddings.weight      |    1536    |\n",
            "|             bert.embeddings.LayerNorm.weight            |    768     |\n",
            "|              bert.embeddings.LayerNorm.bias             |    768     |\n",
            "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.0.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.1.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.2.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.3.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.4.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.5.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.6.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.7.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.8.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.9.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |\n",
            "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |\n",
            "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |\n",
            "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |\n",
            "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |\n",
            "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |\n",
            "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |\n",
            "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |\n",
            "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |\n",
            "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |\n",
            "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |\n",
            "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |\n",
            "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |\n",
            "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |\n",
            "|         bert.encoder.layer.10.output.dense.bias         |    768     |\n",
            "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |\n",
            "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |\n",
            "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |\n",
            "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |\n",
            "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |\n",
            "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |\n",
            "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |\n",
            "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |\n",
            "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |\n",
            "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |\n",
            "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n",
            "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n",
            "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |\n",
            "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |\n",
            "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |\n",
            "|         bert.encoder.layer.11.output.dense.bias         |    768     |\n",
            "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |\n",
            "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |\n",
            "|                    qa_outputs.weight                    |    1536    |\n",
            "|                     qa_outputs.bias                     |     2      |\n",
            "+---------------------------------------------------------+------------+\n",
            "Total Trainable Params: 108893186\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108893186"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model_finetuned_classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM0tZQzDDBl4",
        "outputId": "e6674def-c87e-4738-a7d4-d77aa82affc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------+------------+\n",
            "|                         Modules                         | Parameters |\n",
            "+---------------------------------------------------------+------------+\n",
            "|          bert.embeddings.word_embeddings.weight         |  23440896  |\n",
            "|        bert.embeddings.position_embeddings.weight       |   393216   |\n",
            "|       bert.embeddings.token_type_embeddings.weight      |    1536    |\n",
            "|             bert.embeddings.LayerNorm.weight            |    768     |\n",
            "|              bert.embeddings.LayerNorm.bias             |    768     |\n",
            "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.0.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.1.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.2.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.3.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.4.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.5.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.6.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.7.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.8.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.9.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |\n",
            "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |\n",
            "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |\n",
            "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |\n",
            "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |\n",
            "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |\n",
            "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |\n",
            "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |\n",
            "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |\n",
            "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |\n",
            "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |\n",
            "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |\n",
            "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |\n",
            "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |\n",
            "|         bert.encoder.layer.10.output.dense.bias         |    768     |\n",
            "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |\n",
            "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |\n",
            "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |\n",
            "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |\n",
            "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |\n",
            "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |\n",
            "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |\n",
            "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |\n",
            "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |\n",
            "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |\n",
            "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n",
            "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n",
            "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |\n",
            "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |\n",
            "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |\n",
            "|         bert.encoder.layer.11.output.dense.bias         |    768     |\n",
            "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |\n",
            "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |\n",
            "|                 bert.pooler.dense.weight                |   589824   |\n",
            "|                  bert.pooler.dense.bias                 |    768     |\n",
            "|                    classifier.weight                    |    1536    |\n",
            "|                     classifier.bias                     |     2      |\n",
            "+---------------------------------------------------------+------------+\n",
            "Total Trainable Params: 109483778\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109483778"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3is0MmdGFlhM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}